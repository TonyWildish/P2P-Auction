\section{Introduction}

Run-1 of LHC was a great success for the LHC experiments. The computing models of the experiments were developed and refined over many years, originally having highly segregated network traffic between sites (e.g. \cite{CMSCompModel}). The Tier-0 would transfer only to Tier-1 sites, which would forward data to Tier-2 sites for analysis. Any Tier-2 that wanted data from another Tier-2 would often have to wait for the data to be staged from the source via Tier-1 intermediaries, since direct Tier-2 to Tier-2 transfers were not allowed.

The network played little direct part in those models. In the days before the WLCG grid\cite{WLCG} was established, the network was considered to be an unreliable component, hard to debug when it failed, and a limiting factor for the overall performance of the system. Instead, before and during Run-1, the network turned out to be one of the most reliable and performant components of the grid. Bandwidths available to the experiments were much higher than originally anticipated, and transfer failures were almost invariable associated with a site rather than with the network fabric, and were therefore easily diagnosed.

As a result, the LHC experiments relaxed their computing models to allow transfers between any pair of sites. Tier-2 to Tier-2 transfers accounted for 1/6 of the total network traffic for CMS, for example. Both ATLAS and CMS have created data-federations, based on xrootd\cite{xrootd}, to allow analysis jobs running on any worker node to fall-back to fetching data from any storage element that hosts it, wherever it might be. This extra flexibility in transfers and data-access has had a major impact on the abilities of the experiments to produce physics results, and is expected to be an important component of Run-2.

Despite exceeding expectations, the network is still not yet considered as an active component of the experiments' computing models. It is still used like a utility that is free and infinite, rather than a resource to be scheduled. However, computing models are evolving for Run-2. The experiments face tighter constraints for CPU and storage, and new resource-types (cloud, opportunistic, volunteer) are being integraged into the computing models.

There is now a growing awareness that intelligent use of the network can improve the performance of the entire system\cite{TW_DB}. Speed alone is not the only desirable feature, determinism can be as important, if not more. Nor is it necessary that bandwidth guarantees be hard, soft guarantees that are statistically valid can be equally useful.

For example, the ability to place data with deterministic schedules can lead to more effective scheduling of storage and CPU, by making sure the storage is occupied with data only when and where it is needed. Just-in-time data placement becomes possible, and managing CPU-related deadlines becomes more feasible with it.

Networking groups are making good progress towards providing such capabilities from the network level, through projects such as DYNES\cite{DYNES} and NSI\cite{NSI} which allow the creation of virtual-circuits between sites. The ANSE\cite{ANSE} project has built on their work by integrating the creation of virtual circuits into experiments middleware. As a result, PhEDEx\cite{PhEDEx}, the data-placement management tool of CMS, is now capable of booking and using virtual circuits wherever a suitable interface is available.

N.B. virtual circuits are not the only possible way to provide bandwidth guarantees, other choices are possible, such as the use of multi-path network flows. For convenience, in this paper, the term 'circuit' or 'virtual circuit' will be used to represent any mechanism that can be used to reserve or schedule part of the resources a network can provide.